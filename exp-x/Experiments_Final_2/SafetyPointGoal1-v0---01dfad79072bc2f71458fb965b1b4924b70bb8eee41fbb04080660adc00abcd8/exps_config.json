
{
    "seed": 0,
    "train_cfgs": {
        "total_steps": 600000,
        "torch_threads": 8,
        "vector_env_nums": 4,
        "device": "cuda:0"
    },
    "algo_cfgs": {
        "steps_per_epoch": 20000
    },
    "logger_cfgs": {
        "wandb_project": "Experiments_Final",
        "use_wandb": true,
        "log_dir": "./exp-x\\Experiments_Final_2\\SafetyPointGoal1-v0---01dfad79072bc2f71458fb965b1b4924b70bb8eee41fbb04080660adc00abcd8\\"
    },
    "algo": "PPO",
    "env_id": "SafetyPointGoal1-v0"
}